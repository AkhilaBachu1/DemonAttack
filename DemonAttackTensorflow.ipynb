{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59de2c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5870908d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dqn_model(input_shape, num_actions):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (8, 8), strides=(4, 4), activation='relu', input_shape=input_shape))\n",
    "    model.add(Conv2D(64, (4, 4), strides=(2, 2), activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(num_actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7b86575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DemonAttack-v0 Gym environment\n",
    "env = gym.make('DemonAttack-v0')\n",
    "state_shape = env.observation_space.shape\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8f57060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DQN model\n",
    "model = create_dqn_model(state_shape, num_actions)\n",
    "\n",
    "# Compile the model\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6649a5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperparameters for training\n",
    "num_episodes = 10\n",
    "max_steps_per_episode = 100\n",
    "epsilon = 1.0\n",
    "epsilon_decay = 0.99\n",
    "min_epsilon = 0.01\n",
    "batch_size = 32\n",
    "replay_buffer_size = 100000\n",
    "replay_buffer = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "722feab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Total Reward: 10.0\n",
      "Episode 2, Total Reward: 0.0\n",
      "Episode 3, Total Reward: 10.0\n",
      "Episode 4, Total Reward: 30.0\n",
      "Episode 5, Total Reward: 20.0\n",
      "Episode 6, Total Reward: 10.0\n",
      "Episode 7, Total Reward: 10.0\n",
      "Episode 8, Total Reward: 20.0\n",
      "Episode 9, Total Reward: 20.0\n",
      "Episode 10, Total Reward: 10.0\n"
     ]
    }
   ],
   "source": [
    "# Main training loop\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    for step in range(max_steps_per_episode):\n",
    "        # Choose an action using epsilon-greedy exploration\n",
    "        if tf.random.uniform(()) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            q_values = model.predict(state[None, ...])[0]\n",
    "            action = tf.argmax(q_values).numpy()\n",
    "        \n",
    "        # Take a step in the environment\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        # Add the transition to the replay buffer\n",
    "        replay_buffer.append((state, action, reward, next_state, done))\n",
    "        \n",
    "        # Update the current state and total reward\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Sample a batch from the replay buffer for training\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            batch_indices = tf.random.uniform((batch_size,), minval=0, maxval=len(replay_buffer), dtype=tf.int32)\n",
    "            batch = [replay_buffer[i] for i in batch_indices]\n",
    "            states, actions, rewards, next_states, dones = zip(*batch)\n",
    "            states = tf.stack(states)\n",
    "            actions = tf.constant(actions, dtype=tf.int32)\n",
    "            rewards = tf.constant(rewards, dtype=tf.float32)\n",
    "            next_states = tf.stack(next_states)\n",
    "            dones = tf.constant(dones, dtype=tf.float32)\n",
    "            \n",
    "            # Compute target Q-values using the DQN model\n",
    "            target_q_values = rewards + (1 - dones) * epsilon * tf.reduce_max(model.predict(next_states), axis=1)\n",
    "            \n",
    "            # Compute predicted Q-values for the chosen actions\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_values = model(states)\n",
    "                q_values = tf.reduce_sum(q_values * tf.one_hot(actions, num_actions), axis=1)\n",
    "                # Compute the loss and update the model\n",
    "                loss = tf.reduce_mean(tf.square(target_q_values - q_values))\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Decay epsilon for epsilon-greedy exploration\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "    # Print the total reward for the episode\n",
    "    print(f'Episode {episode + 1}, Total Reward: {total_reward}')              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12793f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "done = False\n",
    "while not done:\n",
    "    q_values = model.predict(state[None, ...])[0]\n",
    "    action = tf.argmax(q_values).numpy()\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0079ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
